
<!-- saved from url=(0101)file:///D:/MyWorks/UoN/Spring/Research%20Method/CW3/literatures/SurVis%20Literature%20Collection.html -->
<html style="overflow: hidden; height: 100%; border: none; padding: 0px; margin: 0px;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <script type="text/javascript" src="./index_files/properties.js.download"></script>
    <script data-main="js/app.js" src="./index_files/require.js.download"></script>
<script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app" src="./index_files/app.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/main" src="./index_files/main.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="jquery" src="./index_files/jquery-1.11.3.min.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/bib" src="./index_files/bib.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/selectors" src="./index_files/selectors.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/stats" src="./index_files/stats.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/timeline" src="./index_files/timeline.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/tags" src="./index_files/tags.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/cluster" src="./index_files/cluster.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/entry_layout" src="./index_files/entry_layout.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/references" src="./index_files/references.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/init_page" src="./index_files/init_page.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="FileSaver" src="./index_files/FileSaver.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="codemirror" src="./index_files/codemirror.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="app/util" src="./index_files/util.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/generated/bib" src="./index_files/bib(1).js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/generated/available_pdf" src="./index_files/available_pdf.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/generated/available_img" src="./index_files/available_img.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/search_stopwords" src="./index_files/search_stopwords.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/tag_categories" src="./index_files/tag_categories.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="data/authorized_tags" src="./index_files/authorized_tags.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="figue" src="./index_files/figue.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="jqueryui" src="./index_files/jquery-ui.min.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="stex" src="./index_files/stex.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="jquery_layout" src="./index_files/jquery.layout-latest.min.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="d3" src="./index_files/d3.v3.min.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="bibtex_js" src="./index_files/bibtex_js.js.download"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="jquery.tooltipster" src="./index_files/jquery.tooltipster.js.download"></script><link rel="stylesheet" type="text/css" href="./index_files/jquery-ui-1.10.4.custom.min.css"><link rel="stylesheet" type="text/css" href="./index_files/codemirror.css"><link rel="stylesheet" type="text/css" href="./index_files/tooltipster.css"><link rel="stylesheet" type="text/css" href="./index_files/tooltipster-survis.css"><link rel="stylesheet" type="text/css" href="./index_files/style.css"><link rel="stylesheet" type="text/css" href="./index_files/selector.css"><link rel="stylesheet" type="text/css" href="./index_files/sparkline.css"><link rel="stylesheet" type="text/css" href="./index_files/timeline.css"><link rel="stylesheet" type="text/css" href="./index_files/entries.css"><title>SurVis Literature Collection</title></head>
<body class="ui-layout-container" style="overflow: hidden; width: auto; height: auto; margin: 0px; position: absolute; inset: 0px;">

<div id="result" class="ui-layout-center ui-layout-pane ui-layout-pane-center" style="position: absolute; margin: 0px; inset: 117px 0px 48px 757px; height: 731px; width: 948px; z-index: 0; padding: 0px; background: rgb(255, 255, 255); border: 1px solid rgb(187, 187, 187); overflow: hidden; display: block; visibility: visible;"><div id="result_header"><div id="stats" class="tooltipstered"><div class="vis sparkline"></div><span>10 publications</span></div><div id="sorting_description">sorted by selector agreement and publication key</div></div><div id="result_body" class="ui-layout-content" style="position: relative; overflow: auto; padding: 10px; height: 669px; visibility: visible;"><div class="entry type_Article" id="agriculture12020232" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/10.3390/agriculture12020232" target="_blank">DOI</a><a href="https://www.mdpi.com/2077-0472/12/2/232" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Research%20on%20Maize%20Seed%20Classification%20and%20Recognition%20Based%20on%20Machine%20Vision%20and%20Deep%20Learning" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Research%20on%20Maize%20Seed%20Classification%20and%20Recognition%20Based%20on%20Machine%20Vision%20and%20Deep%20Learning" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">agriculture12020232</div><div class="series">[Article]</div><div class="year">(2022)</div></div><a class="title" target="_blank" href="http://dx.doi.org/10.3390/agriculture12020232">Research on Maize Seed Classification and Recognition Based on Machine Vision and Deep Learning</a><div class="authors"><div class="author" value="Xu, Peng"><span class="last_name">Xu</span><span class="first_name">,  Peng</span></div><div class="author" value="Tan, Qian"><span class="last_name">Tan</span><span class="first_name">,  Qian</span></div><div class="author" value="Zhang, Yunpeng"><span class="last_name">Zhang</span><span class="first_name">,  Yunpeng</span></div><div class="author" value="Zha, Xiantao"><span class="last_name">Zha</span><span class="first_name">,  Xiantao</span></div><div class="author" value="Yang, Songmei"><span class="last_name">Yang</span><span class="first_name">,  Songmei</span></div><div class="author" value="Yang, Ranbing"><span class="last_name">Yang</span><span class="first_name">,  Ranbing</span></div></div><div class="abstract collapsed" value="Maize is one of the essential crops for food supply. Accurate sorting of seeds is critical for cultivation and marketing purposes, while the traditional methods of variety identification are time-consuming, inefficient, and easily damaged. This study proposes a rapid classification method for maize seeds using a combination of machine vision and deep learning. 8080 maize seeds of five varieties were collected, and then the sample images were classified into training and validation sets in the proportion of 8:2, and the data were enhanced. The proposed improved network architecture, namely P-ResNet, was fine-tuned for transfer learning to recognize and categorize maize seeds, and then it compares the performance of the models. The results show that the overall classification accuracy was determined as 97.91, 96.44, 99.70, 97.84, 98.58, 97.13, 96.59, and 98.28% for AlexNet, VGGNet, P-ResNet, GoogLeNet, MobileNet, DenseNet, ShuffleNet, and EfficientNet, respectively. The highest classification accuracy result was obtained with P-ResNet, and the model loss remained at around 0.01. This model obtained the accuracy of classifications for BaoQiu, ShanCu, XinNuo, LiaoGe, and KouXian varieties, which reached 99.74, 99.68, 99.68, 99.61, and 99.80%, respectively. The experimental results demonstrated that the convolutional neural network model proposed enables the effective classification of maize seeds. It can provide a reference for identifying seeds of other crops and be applied to consumer use and the food industry."><span class="label">Abstract: </span>Maize is one of the essential crops for food supply. Accurate sorting of seeds is critical for cultivation and marketing purposes, while the traditional methods of variety identification are time-consuming, inefficient, and easily damaged. This study proposes a rapid classification method for mai...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="?">?</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="ALTUNTAS2019104874" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2019.104874" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0168169919300481" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Identification%20of%20haploid%20and%20diploid%20maize%20seeds%20using%20convolutional%20neural%20networks%20and%20a%20transfer%20learning%20approach" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Identification%20of%20haploid%20and%20diploid%20maize%20seeds%20using%20convolutional%20neural%20networks%20and%20a%20transfer%20learning%20approach" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">ALTUNTAS2019104874</div><div class="series">Computers and Electronics in Agriculture</div><div class="year">(2019)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2019.104874">Identification of haploid and diploid maize seeds using convolutional neural networks and a transfer learning approach</a><div class="authors"><div class="author" value="Yahya Altuntaş"><span class="name">Yahya Altuntaş</span></div><div class="author" value="Zafer Cömert"><span class="name">Zafer Cömert</span></div><div class="author" value="Adnan Fatih Kocamaz"><span class="name">Adnan Fatih Kocamaz</span></div></div><div class="abstract collapsed" value="Maize is one of the most significant grains cultivated all over the world. Doubled-haploid is an important technique in terms of advanced maize breeding, modern crop improvement and genetic programs, since this technique shortens the breeding period and increases breeding efficiency. However, the selection of the haploid seeds is a major problem of this breeding technique. This process is frequently conducted manually, and this unreliable situation leads to loss of time and labor. Inspired by the recent successes of deep transfer learning, in this study, we approached this problem as a computer vision task to provide a nondestructive, rapid and low-cost model. To achieve this objective, we adopted convolutional neural networks (CNNs) to recognize haploid and diploid maize seeds automatically through a transfer learning approach. More specifically, AlexNet, VVGNet, GoogLeNet, and ResNet were applied for this specific task. The experimental study was carried out using a new dataset consisting of 1230 haploid and 1770 diploid maize seed images. The samples in the dataset were classified considering a marker-assisted selection, known as the R1-nj anthocyanin marker. To measure the success of the CNN models, we utilized several performance metrics, such as accuracy, sensitivity, specificity, quality index, and F-score derived from the confusion matrix and receiver operating characteristic curves. According to the experimental results, the CNN models ensured promising results, and we achieved the most efficient results via VGG-19. The accuracy, sensitivity, specificity, quality index, and F-score of VGG-19 were 94.22%, 94.58%, 93.97%, 94.27%, and 93.07%, respectively. Consequently, the experimental results proved that CNN models can be a useful tool in recognizing haploid maize seeds. Furthermore, we conclude that this approach is significantly superior to machine learning-based methods and conventional manual selection."><span class="label">Abstract: </span>Maize is one of the most significant grains cultivated all over the world. Doubled-haploid is an important technique in terms of advanced maize breeding, modern crop improvement and genetic programs, since this technique shortens the breeding period and increases breeding efficiency. However, the...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="classification">classification</div><div class="tag" value="convolutional neural network">convolutional neural network</div><div class="tag" value="diploid">diploid</div><div class="tag" value="haploid">haploid</div><div class="tag" value="maize">maize</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_Article" id="d14040254" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/10.3390/d14040254" target="_blank">DOI</a><a href="https://www.mdpi.com/1424-2818/14/4/254" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Real-Time%20Classification%20of%20Invasive%20Plant%20Seeds%20Based%20on%20Improved%20YOLOv5%20with%20Attention%20Mechanism" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Real-Time%20Classification%20of%20Invasive%20Plant%20Seeds%20Based%20on%20Improved%20YOLOv5%20with%20Attention%20Mechanism" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">d14040254</div><div class="series">[Article]</div><div class="year">(2022)</div></div><a class="title" target="_blank" href="http://dx.doi.org/10.3390/d14040254">Real-Time Classification of Invasive Plant Seeds Based on Improved YOLOv5 with Attention Mechanism</a><div class="authors"><div class="author" value="Yang, Lianghai"><span class="last_name">Yang</span><span class="first_name">,  Lianghai</span></div><div class="author" value="Yan, Jing"><span class="last_name">Yan</span><span class="first_name">,  Jing</span></div><div class="author" value="Li, Huiru"><span class="last_name">Li</span><span class="first_name">,  Huiru</span></div><div class="author" value="Cao, Xinyue"><span class="last_name">Cao</span><span class="first_name">,  Xinyue</span></div><div class="author" value="Ge, Binjie"><span class="last_name">Ge</span><span class="first_name">,  Binjie</span></div><div class="author" value="Qi, Zhechen"><span class="last_name">Qi</span><span class="first_name">,  Zhechen</span></div><div class="author" value="Yan, Xiaoling"><span class="last_name">Yan</span><span class="first_name">,  Xiaoling</span></div></div><div class="abstract collapsed" value="Seeds of exotic plants transported with imported goods pose a risk of alien species invasion in cross-border transportation and logistics. It is critical to develop stringent inspection and quarantine protocols with active management to control the import and export accompanied by exotic seeds. As a result, a method for promptly identifying exotic plant seeds is urgently needed. In this study, we built a database containing 3000 images of seeds of 12 invasive plants and proposed an improved YOLOv5 target detection algorithm that incorporates a channel attention mechanism. Given that certain seeds in the same family and genus are very similar in appearance and are thus difficult to differentiate, we improved the size model of the initial anchor box to converge better; moreover, we introduce three attention modules, SENet, CBAM, and ECA-Net, to enhance the extraction of globally important features while suppressing the weakening of irrelevant features, thereby effectively solving the problem of automated inspection of similar species. Experiments on an invasive alien plant seed data set reveal that the improved network model fused with ECA-Net requires only a small increase in parameters when compared to the original YOLOv5 network model and achieved greater classification and detection accuracy without affecting detection speed."><span class="label">Abstract: </span>Seeds of exotic plants transported with imported goods pose a risk of alien species invasion in cross-border transportation and logistics. It is critical to develop stringent inspection and quarantine protocols with active management to control the import and export accompanied by exotic seeds. A...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="?">?</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="DEMEDEIROS2021113378" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.indcrop.2021.113378" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0926669021001424" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Deep%20learning-based%20approach%20using%20X-ray%20images%20for%20classifying%20Crambe%20abyssinica%20seed%20quality" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Deep%20learning-based%20approach%20using%20X-ray%20images%20for%20classifying%20Crambe%20abyssinica%20seed%20quality" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">DEMEDEIROS2021113378</div><div class="series">Industrial Crops and Products</div><div class="year">(2021)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.indcrop.2021.113378">Deep learning-based approach using X-ray images for classifying Crambe abyssinica seed quality</a><div class="authors"><div class="author" value="André Dantas {de Medeiros}"><span class="name">André Dantas {de Medeiros}</span></div><div class="author" value="Rodrigo Cupertino Bernardes"><span class="name">Rodrigo Cupertino Bernardes</span></div><div class="author" value="Laércio Junio {da Silva}"><span class="name">Laércio Junio {da Silva}</span></div><div class="author" value="Bruno Antônio Lemos {de Freitas}"><span class="name">Bruno Antônio Lemos {de Freitas}</span></div><div class="author" value="Denise Cunha Fernandes dos Santos Dias"><span class="name">Denise Cunha Fernandes dos Santos Dias</span></div><div class="author" value="Clíssia Barboza {da Silva}"><span class="name">Clíssia Barboza {da Silva}</span></div></div><div class="abstract collapsed" value="The application of imaging technologies combined with state-of-the-art artificial intelligence techniques has provided important advances in the modern oilseed industry. Innovative tools have been designed to improve the characterization of different classes of seeds, and consequently, decision making has become more efficient. This study aimed to assess the potential of deep learning models based on convolutional neural networks (CNN) for monitoring the quality of crambe seeds using X-ray images. In the proposed approach, seeds with different physical and physiological attributes were used to create the models. The models achieved accuracies of 91, 95, and 82 % for discrimination of seeds based on the integrity of internal tissues, germination, and vigor, respectively. Therefore, our findings indicated that digital radiographic images are suitable to provide relevant information on the physical and physiological parameters of crambe seeds. Furthermore, the proposed methodology could be used to classify seeds quickly, non-destructively, and robustly."><span class="label">Abstract: </span>The application of imaging technologies combined with state-of-the-art artificial intelligence techniques has provided important advances in the modern oilseed industry. Innovative tools have been designed to improve the characterization of different classes of seeds, and consequently, decision m...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="convolutional neural networks">convolutional neural networks</div><div class="tag" value="image analysis">image analysis</div><div class="tag" value="oilseed quality">oilseed quality</div><div class="tag" value="physical integrity of seed">physical integrity of seed</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="HUANG2022107393" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2022.107393" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0168169922007013" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Deep%20learning%20based%20soybean%20seed%20classification" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Deep%20learning%20based%20soybean%20seed%20classification" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">HUANG2022107393</div><div class="series">Computers and Electronics in Agriculture</div><div class="year">(2022)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2022.107393">Deep learning based soybean seed classification</a><div class="authors"><div class="author" value="Ziliang Huang"><span class="name">Ziliang Huang</span></div><div class="author" value="Rujing Wang"><span class="name">Rujing Wang</span></div><div class="author" value="Ying Cao"><span class="name">Ying Cao</span></div><div class="author" value="Shijian Zheng"><span class="name">Shijian Zheng</span></div><div class="author" value="Yue Teng"><span class="name">Yue Teng</span></div><div class="author" value="Fenmei Wang"><span class="name">Fenmei Wang</span></div><div class="author" value="Liusan Wang"><span class="name">Liusan Wang</span></div><div class="author" value="Jianming Du"><span class="name">Jianming Du</span></div></div><div class="abstract collapsed" value="Accurately sorting high-quality soybean seeds is a crucial and time-consuming task in quality inspection and food safety. This paper designs a full pipeline to classify the soybean seeds, which follows a segmentation–classification procedure. The image segmentation is performed by a popular deep learning method, the Mask R-CNN, while the classification stage is performed through a novel network, named Soybean Network (SNet). SNet is an extremely lightweight model based on convolutional networks, and it contains mixed feature recalibration (MFR) modules. The MFR module is designed to improve the representation ability of our SNet for damage features so that the model pays more attention to the key regions. Experimental results show that the proposed SNet model achieves 96.2% identification accuracy with only 1.29M parameters, which outperforms six previous state-of-the-art models. The proposed SNet could be used for the automatic recognition of soybean seeds on the resource-limited platform."><span class="label">Abstract: </span>Accurately sorting high-quality soybean seeds is a crucial and time-consuming task in quality inspection and food safety. This paper designs a full pipeline to classify the soybean seeds, which follows a segmentation–classification procedure. The image segmentation is performed by a popular deep ...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="attention mechanism">attention mechanism</div><div class="tag" value="image classification">image classification</div><div class="tag" value="image segmentation">image segmentation</div><div class="tag" value="lightweight convolutional neural networks">lightweight convolutional neural networks</div><div class="tag" value="soybean seed">soybean seed</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="LIN2023106434" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.engappai.2023.106434" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0952197623006188" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Online%20classification%20of%20soybean%20seeds%20based%20on%20deep%20learning" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Online%20classification%20of%20soybean%20seeds%20based%20on%20deep%20learning" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">LIN2023106434</div><div class="series">Engineering Applications of Artificial Intelligence</div><div class="year">(2023)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.engappai.2023.106434">Online classification of soybean seeds based on deep learning</a><div class="authors"><div class="author" value="Wei Lin"><span class="name">Wei Lin</span></div><div class="author" value="Lei Shu"><span class="name">Lei Shu</span></div><div class="author" value="Weibo Zhong"><span class="name">Weibo Zhong</span></div><div class="author" value="Wei Lu"><span class="name">Wei Lu</span></div><div class="author" value="Daoyi Ma"><span class="name">Daoyi Ma</span></div><div class="author" value="Yizhen Meng"><span class="name">Yizhen Meng</span></div></div><div class="abstract collapsed" value="To quickly evaluate soybean quality, we proposed a deep learning-based method for online classification of soybean seeds. Firstly, images of soybean seeds with uneven illumination were segmented based on the multi-scale Retinex with color restoration (MSRCR). Then, a convolutional neural network (CNN) was constructed to achieve soybean seed four-classification with appropriate parameters. The F-score of the normal, damaged, abnormal, and non-classifiable soybeans reached about 95.97%, 97.41%, 97.25%, and 96.14%, respectively. Finally, the method was successfully applied in NVIDIA Jetson TX2 with an accuracy of 95.63% and an average classification time of 4.92 ms for a soybean seed, which can meet the requirement of online soybean quality assessment."><span class="label">Abstract: </span>To quickly evaluate soybean quality, we proposed a deep learning-based method for online classification of soybean seeds. Firstly, images of soybean seeds with uneven illumination were segmented based on the multi-scale Retinex with color restoration (MSRCR). Then, a convolutional neural network ...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="computer vision">computer vision</div><div class="tag" value="convolutional neural networks">convolutional neural networks</div><div class="tag" value="image processing">image processing</div><div class="tag" value="online soybean quality assessment">online soybean quality assessment</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="LODDO2021106269" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2021.106269" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0168169921002866" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=A%20novel%20deep%20learning%20based%20approach%20for%20seed%20image%20classification%20and%20retrieval" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=A%20novel%20deep%20learning%20based%20approach%20for%20seed%20image%20classification%20and%20retrieval" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">LODDO2021106269</div><div class="series">Computers and Electronics in Agriculture</div><div class="year">(2021)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2021.106269">A novel deep learning based approach for seed image classification and retrieval</a><div class="authors"><div class="author" value="Andrea Loddo"><span class="name">Andrea Loddo</span></div><div class="author" value="Mauro Loddo"><span class="name">Mauro Loddo</span></div><div class="author" value="Cecilia {Di Ruberto}"><span class="name">Cecilia {Di Ruberto}</span></div></div><div class="abstract collapsed" value="Seeds image analysis has become essential to preserve biodiversity. This is why recognition and classification of plant species on the earth’s planet is nowadays a great challenge. The paper focuses on this purpose by studying two plant seeds datasets to classify their families or species through deep learning techniques. SeedNet, a novel CNN has been proposed to face the depicted issue, and several state-of-the-art convolutional neural networks have been exploited for an exhaustive comparison of most adequate for the considered scenario. In detail, promising results in seed classification for both analysed datasets, reaching accuracy values of 95.65% for the first one and 97.47% for the second one, have been obtained. The retrieval problem with the deep learning approach was also addressed, achieving satisfying performances. We consider the obtained results for both the tasks as an excellent starting point to develop a complete seeds recognition, classification and retrieval system to offer impressive support in agriculture and botany fields."><span class="label">Abstract: </span>Seeds image analysis has become essential to preserve biodiversity. This is why recognition and classification of plant species on the earth’s planet is nowadays a great challenge. The paper focuses on this purpose by studying two plant seeds datasets to classify their families or species through...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="agriculture">agriculture</div><div class="tag" value="deep learning">deep learning</div><div class="tag" value="image analysis">image analysis</div><div class="tag" value="machine learning">machine learning</div><div class="tag" value="retrieval">retrieval</div><div class="tag" value="seeds classification">seeds classification</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_article" id="PRZYBYLO2019490" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2018.12.001" target="_blank">DOI</a><a href="https://www.sciencedirect.com/science/article/pii/S0168169918313930" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Using%20Deep%20Convolutional%20Neural%20Network%20for%20oak%20acorn%20viability%20recognition%20based%20on%20color%20images%20of%20their%20sections" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Using%20Deep%20Convolutional%20Neural%20Network%20for%20oak%20acorn%20viability%20recognition%20based%20on%20color%20images%20of%20their%20sections" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">PRZYBYLO2019490</div><div class="series">Computers and Electronics in Agriculture</div><div class="year">(2019)</div></div><a class="title" target="_blank" href="http://dx.doi.org/https://doi.org/10.1016/j.compag.2018.12.001">Using Deep Convolutional Neural Network for oak acorn viability recognition based on color images of their sections</a><div class="authors"><div class="author" value="Jaromir Przybyło"><span class="name">Jaromir Przybyło</span></div><div class="author" value="Mirosław Jabłoński"><span class="name">Mirosław Jabłoński</span></div></div><div class="abstract collapsed" value="Convolutional Neural Networks (CNNs) are essential tools in many image recognition tasks. In this article we propose using a Deep Convolutional Neural Network for the task of visual assessment of the viability of mechanically scarified quercus robur l. seeds. Currently, this work is mainly performed by mechanical scarification followed by optical assessment by an employee. Here, we focus on acorn classification based on an innovative feature which is colour and intensity of image of sections of the seeds. We show that deep network accuracy (85%) is comparable or slightly higher than manual assessment of the viability of oak seeds. Also, we explore the impact of various image representations (colour, entropy, edges) as well as network architecture and its parameters on the classification results. We found that the image representation is a key factor in what the CNN is learning (topography of a pathological change or colour/intensity information only)."><span class="label">Abstract: </span>Convolutional Neural Networks (CNNs) are essential tools in many image recognition tasks. In this article we propose using a Deep Convolutional Neural Network for the task of visual assessment of the viability of mechanically scarified quercus robur l. seeds. Currently, this work is mainly perfor...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="cnn">cnn</div><div class="tag" value="convolutional neural networks">convolutional neural networks</div><div class="tag" value="deep learning">deep learning</div><div class="tag" value="image recognition">image recognition</div><div class="tag" value="viability of oak seeds">viability of oak seeds</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_Article" id="sym12122018" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/10.3390/sym12122018" target="_blank">DOI</a><a href="https://www.mdpi.com/2073-8994/12/12/2018" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=A%20Convolution%20Neural%20Network-Based%20Seed%20Classification%20System" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=A%20Convolution%20Neural%20Network-Based%20Seed%20Classification%20System" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">sym12122018</div><div class="series">[Article]</div><div class="year">(2020)</div></div><a class="title" target="_blank" href="http://dx.doi.org/10.3390/sym12122018">A Convolution Neural Network-Based Seed Classification System</a><div class="authors"><div class="author" value="Gulzar, Yonis"><span class="last_name">Gulzar</span><span class="first_name">,  Yonis</span></div><div class="author" value="Hamid, Yasir"><span class="last_name">Hamid</span><span class="first_name">,  Yasir</span></div><div class="author" value="Soomro, Arjumand Bano"><span class="last_name">Soomro</span><span class="first_name">,  Arjumand Bano</span></div><div class="author" value="Alwan, Ali A."><span class="last_name">Alwan</span><span class="first_name">,  Ali A.</span></div><div class="author" value="Journaux, Ludovic"><span class="last_name">Journaux</span><span class="first_name">,  Ludovic</span></div></div><div class="abstract collapsed" value="Over the last few years, the research into agriculture has gained momentum, showing signs of rapid growth. The latest to appear on the scene is bringing convenience in how agriculture can be done by employing various computational technologies. There are lots of factors that affect agricultural production, with seed quality topping the list. Seed classification can provide additional knowledge about quality production, seed quality control and impurity identification. The process of categorising seeds has been traditionally done based on characteristics like colour, shape and texture. Generally, this is performed by specialists by visually inspecting each sample, which is a very tedious and time-consuming task. This procedure can be easily automated, providing a significantly more efficient method for seed sorting than having them be inspected using human labour. In related areas, computer vision technology based on machine learning (ML), symmetry and, more particularly, convolutional neural networks (CNNs) have been generously applied, often resulting in increased work efficiency. Considering the success of the computational intelligence methods in other image classification problems, this research proposes a classification system for seeds by employing CNN and transfer learning. The proposed system contains a model that classifies 14 commonly known seeds with the implication of advanced deep learning techniques. The techniques applied in this research include decayed learning rate, model checkpointing and hybrid weight adjustment. This research applies symmetry when sampling the images of the seeds during data formation. The application of symmetry generates homogeneity with regards to resizing and labelling the images to extract their features. This resulted in 99% classification accuracy during the training set. The proposed model produced results with an accuracy of 99% for the test set, which contained 234 images. These results were much higher than the results reported in related research."><span class="label">Abstract: </span>Over the last few years, the research into agriculture has gained momentum, showing signs of rapid growth. The latest to appear on the scene is bringing convenience in how agriculture can be done by employing various computational technologies. There are lots of factors that affect agricultural p...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="?">?</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div><div class="entry type_Article" id="sym13101892" style="border-color: rgb(200, 200, 200);"><div class="entry_main"><div class="links"><a href="http://dx.doi.org/10.3390/sym13101892" target="_blank">DOI</a><a href="https://www.mdpi.com/2073-8994/13/10/1892" target="_blank">URL</a><a href="http://scholar.google.de/scholar?hl=en&amp;q=Performance%20of%20Various%20Deep-Learning%20Networks%20in%20the%20Seed%20Classification%20Problem" target="_blank">Google Scholar</a><a href="https://www.google.de/search?q=Performance%20of%20Various%20Deep-Learning%20Networks%20in%20the%20Seed%20Classification%20Problem" target="_blank">Google</a></div><div class="entry_header"><div class="vis sparkline tooltipstered"></div><div class="id">sym13101892</div><div class="series">[Article]</div><div class="year">(2021)</div></div><a class="title" target="_blank" href="http://dx.doi.org/10.3390/sym13101892">Performance of Various Deep-Learning Networks in the Seed Classification Problem</a><div class="authors"><div class="author" value="Eryigit, Recep"><span class="last_name">Eryigit</span><span class="first_name">,  Recep</span></div><div class="author" value="Tugrul, Bulent"><span class="last_name">Tugrul</span><span class="first_name">,  Bulent</span></div></div><div class="abstract collapsed" value="We report the results of an in-depth study of 15 variants of five different Convolutional Neural Network (CNN) architectures for the classification of seeds of seven different grass species that possess symmetry properties. The performance metrics of the nets are investigated in relation to the computational load and the number of parameters. The results indicate that the relation between the accuracy performance and operation count or number of parameters is linear in the same family of nets but that there is no relation between the two when comparing different CNN architectures. Using default pre-trained weights of the CNNs was found to increase the classification accuracy by ≈3% compared with training from scratch. The best performing CNN was found to be DenseNet201 with a 99.42% test accuracy for the highest resolution image set."><span class="label">Abstract: </span>We report the results of an in-depth study of 15 variants of five different Convolutional Neural Network (CNN) architectures for the classification of seeds of seven different grass species that possess symmetry properties. The performance metrics of the nets are investigated in relation to the c...<span class="expand"> &gt; </span></div><div class="tags"><span class="tag_list"><div class="tag" value="type:?"><span class="tag_category">type:</span>?</div><div class="tag" value="?">?</div></span><form class="add_tag button tooltip tooltipstered">+</form></div></div><div class="footer_container"><div class="button select_similar tooltip tooltipstered">select similar</div><div class="bibtex_control button tooltip tooltipstered">BibTeX </div></div></div></div></div><div id="header" class="ui-layout-north ui-layout-pane ui-layout-pane-north" style="position: absolute; margin: 0px; inset: 0px 0px auto; width: auto; z-index: 3; padding: 10px; background: rgb(255, 255, 255); border: 1px solid rgb(187, 187, 187); overflow: visible; height: 90px; display: block; visibility: visible;"><div id="title"><h1>SurVis Literature Collection</h1><div id="paper">Sample literature collection for <b><a href="http://www.visus.uni-stuttgart.de/uploads/tx_vispublications/eurovis14-star.pdf" target="_blank">SurVis</a></b><div class="button">select</div></div><a href="https://github.com/fabian-beck/survis"><img title="SurVis 0.1.0" src="./index_files/survis_small.png"></a></div><form id="search"> <input type="search" placeholder="search ..."> <div class="button" id="search_button">search </div> </form><div id="selectors_container"><div class="label">Selectors</div><div id="selectors"></div><div class="selector selector0" id="selector0"><div class="selector_type"></div></div><div class="selector selector1" id="selector1"><div class="selector_type"></div></div><div class="selector selector2" id="selector2"><div class="selector_type"></div></div><div class="selector selector3" id="selector3"><div class="selector_type"></div></div><div class="selector selector4" id="selector4"><div class="selector_type"></div></div><div class="selector selector5" id="selector5"><div class="selector_type"></div></div><div id="clear_selectors" class="button tooltip tooltipstered">clear</div><div style="clear: both;"></div></div></div><div id="footer" class="ui-layout-south ui-layout-pane ui-layout-pane-south" style="position: absolute; margin: 0px; inset: auto 0px 0px; width: auto; z-index: 0; padding: 10px; background: rgb(255, 255, 255); border: 1px solid rgb(187, 187, 187); overflow: auto; height: 21px; display: block; visibility: visible;"><div id="extra_pages_list"><div class="button">about</div></div><div id="menu"><div id="export_bibtex" class="button tooltip tooltipstered"><span class="symbol">B</span>download BibTeX</div><div id="add_entry" class="button tooltip tooltipstered"><span class="symbol">+</span>add entries</div><div id="save" class="button tooltip tooltipstered"><span class="symbol">D</span>save to local storage</div><a href="file:///D:/MyWorks/UoN/Spring/Research%20Method/CW3/survis/survis_0.1.0_20151022/src/index.html?loadFromLocalStorage=true"><div id="load_local" class="button tooltip tooltipstered"><span class="symbol">R</span>load from local storage</div></a><a href="file:///D:/MyWorks/UoN/Spring/Research%20Method/CW3/survis/survis_0.1.0_20151022/src/index.html"><div id="load_default" class="button tooltip tooltipstered"><span class="symbol">R</span>load default</div></a></div></div><div id="control" class="ui-layout-west ui-layout-pane ui-layout-pane-west ui-layout-pane-hover ui-layout-pane-west-hover ui-layout-pane-open-hover ui-layout-pane-west-open-hover" style="position: absolute; margin: 0px; inset: 117px auto 48px 0px; height: 711px; z-index: 0; padding: 10px; background: rgb(255, 255, 255); border: 1px solid rgb(187, 187, 187); overflow: auto; width: 730px; display: block; visibility: visible;"><div id="timeline-container"><h2><span class="symbol">/</span>Timeline</h2><div id="timeline" class="toggle-container"><div class="label">publications per year</div><svg class="chart" height="100px" style="border: 1px solid black;" width="710px"><rect class="periodUneven" shape-rendering="crispEdges" x="0" y="-1" width="142" height="102" style="fill: rgb(204, 204, 204);"></rect><text class="periodUneven" x="1" y="20" style="font-size: 14pt; fill: rgb(255, 255, 255);">2019</text><rect class="periodEven" shape-rendering="crispEdges" x="142" y="-1" width="142" height="102" style="fill: rgb(255, 255, 255);"></rect><text class="periodEven" x="143" y="20" style="font-size: 14pt; fill: rgb(204, 204, 204);">2020</text><rect class="periodUneven" shape-rendering="crispEdges" x="284" y="-1" width="142" height="102" style="fill: rgb(204, 204, 204);"></rect><text class="periodUneven" x="285" y="20" style="font-size: 14pt; fill: rgb(255, 255, 255);">2021</text><rect class="periodEven" shape-rendering="crispEdges" x="426" y="-1" width="142" height="102" style="fill: rgb(255, 255, 255);"></rect><text class="periodEven" x="427" y="20" style="font-size: 14pt; fill: rgb(204, 204, 204);">2022</text><rect class="periodUneven" shape-rendering="crispEdges" x="568" y="-1" width="142" height="102" style="fill: rgb(204, 204, 204);"></rect><text class="periodUneven" x="569" y="20" style="font-size: 14pt; fill: rgb(255, 255, 255);">2023</text><line x1="0" y1="75" x2="710" y2="75" shape-rendering="crispEdges" stroke-opacity="0.15" style="stroke: black; stroke-width: 1px;"></line><text x="0" y="87" style="font-size: 12pt;">1</text><line x1="0" y1="50" x2="710" y2="50" shape-rendering="crispEdges" stroke-opacity="0.15" style="stroke: black; stroke-width: 1px;"></line><text x="0" y="62" style="font-size: 12pt;">2</text><line x1="0" y1="25" x2="710" y2="25" shape-rendering="crispEdges" stroke-opacity="0.15" style="stroke: black; stroke-width: 1px;"></line><text x="0" y="37" style="font-size: 12pt;">3</text><rect class="bar total tooltip" shape-rendering="crispEdges" x="0" y="50" width="142" height="51" style="fill: rgb(238, 238, 238); stroke: black;"></rect><rect class="bar total tooltip" shape-rendering="crispEdges" x="142" y="75" width="142" height="26" style="fill: rgb(238, 238, 238); stroke: black;"></rect><rect class="bar total tooltip" shape-rendering="crispEdges" x="284" y="25" width="142" height="76" style="fill: rgb(238, 238, 238); stroke: black;"></rect><rect class="bar total tooltip" shape-rendering="crispEdges" x="426" y="25" width="142" height="76" style="fill: rgb(238, 238, 238); stroke: black;"></rect><rect class="bar total tooltip" shape-rendering="crispEdges" x="568" y="75" width="142" height="26" style="fill: rgb(238, 238, 238); stroke: black;"></rect></svg></div></div><div id="tag_clouds"><div class="tag_cloud" id="tag_cloud_keywords"><div class="tag_occurrence toggle-container">min<div class="button dec small">-</div><span>1</span><div class="button inc small">+</div></div><form class="tag_cloud_filter toggle-container"><input type="search" placeholder="filter ..."></form><h2><span class="symbol">/</span>Keywords</h2><div class="tags-container toggle-container"><div class="tag_category"><span class="label tooltip tooltipstered">type: </span><div class="tag freq10" value="10"><div class="vis sparkline"></div><span class="text">?</span><span class="tag_frequency">10</span></div></div><div class="tag_category"><span class="label tooltip tooltipstered">other: </span><div class="tag freq3" value="4"><div class="vis sparkline"></div><span class="text">?</span><span class="tag_frequency">4</span></div><div class="tag freq3" value="3"><div class="vis sparkline"></div><span class="text">convolutional neural networks</span><span class="tag_frequency">3</span></div><div class="tag freq2" value="2"><div class="vis sparkline"></div><span class="text">image analysis</span><span class="tag_frequency">2</span></div><div class="tag freq2" value="2"><div class="vis sparkline"></div><span class="text">deep learning</span><span class="tag_frequency">2</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">classification</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">convolutional neural network</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">diploid</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">haploid</span><span class="tag_frequency">1</span></div><div class="tag freq1 tooltipstered" value="1"><div class="vis sparkline"></div><span class="text">maize</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">oilseed quality</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">physical integrity of seed</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">attention mechanism</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">image classification</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">image segmentation</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">lightweight convolutional neural networks</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">soybean seed</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">computer vision</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">image processing</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">online soybean quality assessment</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">agriculture</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">machine learning</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">retrieval</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">seeds classification</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">cnn</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">image recognition</span><span class="tag_frequency">1</span></div><div class="tag freq1" value="1"><div class="vis sparkline"></div><span class="text">viability of oak seeds</span><span class="tag_frequency">1</span></div></div></div></div><div class="tag_cloud" id="tag_cloud_author"><div class="tag_occurrence toggle-container">min<div class="button dec small">-</div><span>1</span><div class="button inc small">+</div></div><form class="tag_cloud_filter toggle-container"><input type="search" placeholder="filter ..."></form><h2><span class="symbol">/</span>Authors</h2><div class="tags-container toggle-container"><div class="tag_category"><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Xu, Peng</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Tan, Qian</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Zhang, Yunpeng</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Zha, Xiantao</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yang, Songmei</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yang, Ranbing</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yahya Altuntaş</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Zafer Cömert</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Adnan Fatih Kocamaz</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yang, Lianghai</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yan, Jing</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Li, Huiru</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Cao, Xinyue</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Ge, Binjie</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Qi, Zhechen</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yan, Xiaoling</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">André Dantas de Medeiros</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Rodrigo Cupertino Bernardes</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Laércio Junio da Silva</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Bruno Antônio Lemos de Freitas</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Denise Cunha Fernandes dos Santos Dias</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Clíssia Barboza da Silva</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Ziliang Huang</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Rujing Wang</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Ying Cao</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Shijian Zheng</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yue Teng</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Fenmei Wang</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Liusan Wang</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Jianming Du</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized tooltipstered" value="1"><div class="vis sparkline"></div><span class="text">Wei Lin</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Lei Shu</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Weibo Zhong</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Wei Lu</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Daoyi Ma</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Yizhen Meng</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Andrea Loddo</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Mauro Loddo</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized tooltipstered" value="1"><div class="vis sparkline"></div><span class="text">Cecilia Di Ruberto</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Jaromir Przybyło</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Mirosław Jabłoński</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Gulzar, Yonis</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Hamid, Yasir</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Soomro, Arjumand Bano</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized tooltipstered" value="1"><div class="vis sparkline"></div><span class="text">Alwan, Ali A.</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Journaux, Ludovic</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Eryigit, Recep</span><span class="tag_frequency">1</span></div><div class="tag freq1 authorized" value="1"><div class="vis sparkline"></div><span class="text">Tugrul, Bulent</span><span class="tag_frequency">1</span></div></div></div></div><div class="tag_cloud" id="tag_cloud_series"><div class="tag_occurrence toggle-container">min<div class="button dec small">-</div><span>1</span><div class="button inc small">+</div></div><form class="tag_cloud_filter toggle-container"><input type="search" placeholder="filter ..."></form><h2><span class="symbol">/</span>Series</h2><div class="tags-container toggle-container"><div class="tag_category"><div class="tag freq10 authorized" value="10"><div class="vis sparkline"></div><span class="text">?</span><span class="tag_frequency">10</span></div></div></div></div></div><div id="clusters" class="tag_cloud"><h2><span class="symbol">/</span>Clusters</h2><div id="clusterings" class="toggle-container"></div><div id="clustering_controls" class="toggle-container"><span class="label">new clustering:</span><div class="n_clusters">number of clusters<div class="button dec small">-</div><span>5</span><div class="button inc small">+</div></div><div class="clustering_criteria"><input id="keywords_checkbox" type="checkbox" checked="checked"><span>keywords</span><input id="author_checkbox" type="checkbox"><span>authors</span></div><div id="create_clustering" class="button tooltip tooltipstered">create clustering</div></div></div></div><div id="" class="ui-layout-resizer ui-layout-resizer-north ui-draggable-handle ui-layout-resizer-open ui-layout-resizer-north-open" title="Resize" style="position: absolute; padding: 0px; margin: 0px; font-size: 1px; text-align: left; overflow: hidden; z-index: 2; background: rgb(221, 221, 221); border: none; cursor: n-resize; top: 111px; width: 1707px; height: 6px; left: 0px;"></div><div id="" class="ui-layout-resizer ui-layout-resizer-south ui-layout-resizer-open ui-layout-resizer-south-open" style="position: absolute; padding: 0px; margin: 0px; font-size: 1px; text-align: left; overflow: hidden; z-index: 2; background: rgb(221, 221, 221); border: none; bottom: 42px; cursor: default; width: 1707px; height: 6px; left: 0px;"></div><div id="" class="ui-layout-resizer ui-layout-resizer-west ui-draggable-handle ui-layout-resizer-open ui-layout-resizer-west-open" title="Resize" style="position: absolute; padding: 0px; margin: 0px; font-size: 1px; text-align: left; overflow: hidden; z-index: 2; background: rgb(221, 221, 221); border: none; cursor: w-resize; left: 751px; height: 732px; width: 6px; top: 117px;"><div id="" class="ui-layout-toggler ui-layout-toggler-west ui-layout-toggler-open ui-layout-toggler-west-open" title="Close" style="position: absolute; display: block; padding: 0px; margin: 0px; overflow: hidden; text-align: center; font-size: 1px; cursor: pointer; z-index: 1; background: rgb(170, 170, 170); visibility: visible; height: 50px; width: 6px; top: 341px; left: 0px;"></div></div></body></html>
